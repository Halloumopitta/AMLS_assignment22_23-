{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "48d97a6a",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Library imports\n",
    "import os\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn  \n",
    "import torch.optim as optim  \n",
    "import torchvision.transforms as transforms  \n",
    "import torchvision\n",
    "from torch.utils.data import (Dataset,DataLoader) \n",
    "\n",
    "from PIL import Image\n",
    "from skimage import io\n",
    "\n",
    "import time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "8410c119",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Andreas\\Desktop\\AMLS_22-23_SN18086046\n",
      " cpu being used\n"
     ]
    }
   ],
   "source": [
    "directory = os.getcwd() # Path of the current working directory.\n",
    "print(directory)\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "print(f\" {device} being used\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "058673aa",
   "metadata": {},
   "outputs": [],
   "source": [
    "num_epochs = 5 #complete passess over the data set\n",
    "num_classes = 5 #five types of face shapes/eye color\n",
    "batch_size = 200 #data must be loaded in batches for more efficient training (high batch size can lead to memory overload)\n",
    "learning_rate = 0.01"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "a5e4fcda",
   "metadata": {},
   "outputs": [],
   "source": [
    "class cartoonDataset(Dataset):\n",
    "    def __init__(self,csv_file,root_dir,transform=None):\n",
    "        self.annotations=pd.read_csv(csv_file)\n",
    "        self.root_dir=root_dir\n",
    "        self.transform=transform\n",
    "        pass\n",
    "  \n",
    "    def __len__(self):\n",
    "        \n",
    "        return len(self.annotations)\n",
    "        pass\n",
    "    def __getitem__(self, index): #PyTorch chooses the index\n",
    "        img_path=os.path.join(self.root_dir, self.annotations.iloc[index,0]) #row 'index' and column 0\n",
    "        image=io.imread(img_path)\n",
    "        y_label=torch.tensor(int(self.annotations.iloc[index,1]))\n",
    "        yt=y_label.type(torch.LongTensor)\n",
    "        if self.transform:\n",
    "            image=self.transform(image)\n",
    "        return (image,yt)\n",
    "        \n",
    "        pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "061e0b18",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train dataset size: 4000\n",
      "Validation dataset size: 1000\n",
      "200\n"
     ]
    }
   ],
   "source": [
    "#reduce image size\n",
    "tf=transforms.Compose([\n",
    "    transforms.ToPILImage(),\n",
    "    transforms.Resize((250,250)), #from [500,500]\n",
    "    transforms.ToTensor()\n",
    "])\n",
    "dataset=cartoonDataset(csv_file='dataset_AMLS_22-23/cartoon_set/face_shape_labels.csv',\n",
    "                      root_dir='dataset_AMLS_22-23/cartoon_set/img',transform=tf)                                                                                            \n",
    "                                                                                            \n",
    "#split the images into test and validation sets\n",
    "partial_dataset,discard_dataset=torch.utils.data.random_split(dataset,[5000,5000]) #get 5000 random images\n",
    "train_dataset,validation_dataset=torch.utils.data.random_split(partial_dataset,[4000,1000]) #split the images \n",
    "train_loader=DataLoader(dataset=train_dataset,batch_size=200,shuffle=True)\n",
    "validation_loader=DataLoader(dataset=validation_dataset,batch_size=200,shuffle=False)\n",
    "#shuffle=True to ensure our model is not biased for some categories \n",
    "print('Train dataset size:', len(train_dataset)) #must give 4000\n",
    "print('Validation dataset size:',len(validation_dataset)) #must give 1000\n",
    "print(batch_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "92d8d642",
   "metadata": {},
   "outputs": [],
   "source": [
    "#CNN model\n",
    "class convNet(nn.Module):\n",
    "    def __init__(self,num_classes=5):\n",
    "        super(convNet,self).__init__()\n",
    "        \n",
    "        #Input shape =(batch_size,RGB_channel,Image dimensions)=(200,3,500,500)\n",
    "        #The first convolutional layer\n",
    "        self.conv1=nn.Conv2d(in_channels=4,out_channels=12,kernel_size=3,stride=1,padding=1)\n",
    "        #New shape from formula (width-kernel+2P)/s +1 ->Shape=(200,12,250,250)\n",
    "        self.bn1=nn.BatchNorm2d(num_features=12) #normalisation\n",
    "        #Shape=(200,12,250,250)\n",
    "        self.relu1=nn.ReLU()\n",
    "        #Shape=(200,12,500,500)\n",
    "        \n",
    "        self.pool=nn.MaxPool2d(kernel_size=2)\n",
    "        #Reduce the image size be factor 2\n",
    "        #Shape= (200,12,250,250)\n",
    "        \n",
    "        #second convolutional layer\n",
    "        self.conv2=nn.Conv2d(in_channels=12,out_channels=20,kernel_size=3,stride=1,padding=1)\n",
    "        #Shape= (100,20,89,109)\n",
    "        self.relu2=nn.ReLU()\n",
    "        #Shape= (100,20,89,109)\n",
    "        \n",
    "        \n",
    "        #Third convolutional layer\n",
    "        self.conv3=nn.Conv2d(in_channels=20,out_channels=32,kernel_size=3,stride=1,padding=1)\n",
    "        #Shape= (100,32,89,109)\n",
    "        self.bn3=nn.BatchNorm2d(num_features=32)\n",
    "        #Shape= (100,32,89,109)\n",
    "        self.relu3=nn.ReLU()\n",
    "        #Shape= (100,32,89,109)\n",
    "        \n",
    "        #fully connected layer\n",
    "        self.fc=nn.Linear(in_features=125*125* 32,out_features=num_classes)\n",
    "        \n",
    "        \n",
    "        #feed forward function\n",
    "    def forward(self,input):\n",
    "        output=self.conv1(input)\n",
    "        output=self.bn1(output)\n",
    "        output=self.relu1(output)\n",
    "            \n",
    "        output=self.pool(output)\n",
    "            \n",
    "        output=self.conv2(output)\n",
    "        output=self.relu2(output)\n",
    "        \n",
    "            \n",
    "        output=self.conv3(output)\n",
    "        output=self.bn3(output)\n",
    "        output=self.relu3(output)\n",
    "            \n",
    "            \n",
    "        #Above output will be in matrix form, with shape (100,32,112,112)\n",
    "            \n",
    "        output=output.view(-1,32*125*125)\n",
    "            \n",
    "            \n",
    "        output=self.fc(output)\n",
    "            \n",
    "        return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "e17cb0e9",
   "metadata": {},
   "outputs": [],
   "source": [
    "model=convNet(num_classes=5).to(device) #send it to cuda/cpu\n",
    "\n",
    "#Loss and optimizer functions\n",
    "optimizer=torch.optim.Adam(model.parameters(),lr=0.001,weight_decay=0.0001)\n",
    "criterion=nn.CrossEntropyLoss()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "6074f2f3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cost at epoch 0 is 46.87455826997757\n",
      "Cost at epoch 1 is 2.733470377326012\n",
      "Cost at epoch 2 is 0.3335039782919921\n",
      "Cost at epoch 3 is 0.07996633845614269\n",
      "Cost at epoch 4 is 0.028385126073385437\n",
      "Training set accuracy....\n",
      " 3989 / 4000 correct images with accuracy 99.72%\n",
      "Validation set accuracy....\n",
      " 993 / 1000 correct images with accuracy 99.30%\n",
      "Time taken: 1484.8630084991455 seconds\n"
     ]
    }
   ],
   "source": [
    "# Train Network\n",
    "start_time=time.time()\n",
    "\n",
    "for epoch in range(num_epochs):\n",
    "    losses = []\n",
    "\n",
    "    for batch_idx, (data, targets) in enumerate(train_loader):\n",
    "        # Get data to cuda if possible\n",
    "        data = data.to(device=device)\n",
    "        targets = targets.to(device=device)\n",
    "\n",
    "        # forward pass (Evaluation)\n",
    "        output = model(data)\n",
    "        loss = criterion(output, targets)\n",
    "\n",
    "        losses.append(loss.item())\n",
    "\n",
    "        # backward pass (Optimization)\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "\n",
    "        # adam step\n",
    "        optimizer.step()\n",
    "\n",
    "    print(f\"Cost at epoch {epoch} is {sum(losses)/len(losses)}\")\n",
    "\n",
    "# Check accuracy on training to see how good our model is\n",
    "def check_accuracy(loader, model):\n",
    "    num_correct = 0\n",
    "    num_total= 0\n",
    "    \n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        for images, labels in loader:\n",
    "            images = images.to(device=device)\n",
    "            labbels = labels.to(device=device)\n",
    "\n",
    "            output = model(images)\n",
    "            _, predictions = output.max(1)\n",
    "            num_correct += (predictions == labels).sum()\n",
    "            num_total += predictions.size(0)\n",
    "\n",
    "        print( f\" {num_correct} / {num_total} correct images with accuracy {float(num_correct)/float(num_total)*100:.2f}%\")\n",
    "\n",
    "    model.train()\n",
    "\n",
    "\n",
    "print(\"Training set accuracy....\")\n",
    "check_accuracy(train_loader, model)\n",
    "    \n",
    "print(\"Validation set accuracy....\")\n",
    "check_accuracy(validation_loader, model)  \n",
    "\n",
    "elapsed_time=time.time()\n",
    "print('Time taken:',(elapsed_time-start_time),'seconds')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "223e13c2",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ff1183dd",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
